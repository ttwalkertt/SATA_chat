{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Storage-Focused Chatbot\n",
    "Answer all your SAS, SATA, and NVMe questions\n",
    "\n",
    "ref: \n",
    "- https://www.mlq.ai/gpt-4-pinecone-website-ai-assistant/\n",
    "- https://github.com/rabbitmetrics/langchain-13-min/blob/main/notebooks/langchain-13-min.ipynb\n",
    "- https://blog.futuresmart.ai/building-a-document-based-question-answering-system-with-langchain-pinecone-and-llms-like-gpt-4-and-chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "\n",
    "print(f\"current directory: {os.getcwd()}\")\n",
    " \n",
    "if find_dotenv():\n",
    "    print(\"Found .env file\")\n",
    "    load_dotenv(find_dotenv())  \n",
    "else:\n",
    "    print(\"No .env file found\")\n",
    "# print all environment variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import schema for chat messages and ChatOpenAI in order to query chatmodels GPT-3.5-turbo or GPT-4\n",
    "\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\",temperature=0.3)\n",
    "# messages = [\n",
    "#     SystemMessage(content=\"You are an expert Linux storage engineer with experience in SATA, SAS, hard drives, NVMe, and SSDs\"),\n",
    "#     HumanMessage(content=\"give a short description of your skills\")\n",
    "# ]\n",
    "# response=chat(messages)\n",
    "\n",
    "# print(response.content,end='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"create embeddings for langchain documents\n",
    "Here we are extracting the text and metadata into all_the_pages\"\"\"\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "\n",
    "files = os.listdir(\"sources\")\n",
    "print(files)\n",
    "\n",
    "all_the_pages = []\n",
    "\n",
    "for file in files:\n",
    "    loader = PyMuPDFLoader(f\"sources/{file}\")\n",
    "    pages = loader.load_and_split()\n",
    "    all_the_pages.extend(pages)\n",
    "\n",
    "\n",
    "print(f\"len(all_the_pages) = {len(all_the_pages)}\")\n",
    "# split it into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 200,\n",
    "    chunk_overlap  = 10,\n",
    ")\n",
    "docs = text_splitter.split_documents(all_the_pages)\n",
    "print(f\"len(docs) = {len(docs)}\")\n",
    "\n",
    "\n",
    "# create embeddings\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and instantiate OpenAI embeddings\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model_name=\"text-embedding-ada-002\")\n",
    "#embeddings = OpenAIEmbeddings(model_name=\"gpt-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import and initialize Pinecone client\n",
    "\n",
    "import os\n",
    "import tiktoken\n",
    "import pinecone\n",
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "\n",
    "pinecone.init(\n",
    "    api_key=os.getenv('PINECONE_API_KEY'),  \n",
    "    environment=os.getenv('PINECONE_ENV')  \n",
    ")\n",
    "   \n",
    "index_name = \"sata-chat\"\n",
    "print(f\"deleting index {index_name}\")\n",
    "pinecone.delete_index(index_name)\n",
    "print(f\"creating index {index_name}\")\n",
    "pinecone.create_index(index_name, dimension=1536)\n",
    "print(f\"indexing {index_name}\")\n",
    "index = Pinecone.from_documents(docs, embeddings, index_name=index_name)\n",
    "pinecone.list_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "\n",
    "def get_similiar_docs(query, k=2, score=False):\n",
    "  if score:\n",
    "    similar_docs = index.similarity_search_with_score(query, k=k)\n",
    "  else:\n",
    "    similar_docs = index.similarity_search(query, k=k)\n",
    "  return similar_docs\n",
    "\n",
    "model_name = \"gpt-4\"\n",
    "model_name = \"gpt-3.5-turbo\"\n",
    "llm = ChatOpenAI(model_name=model_name)\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm,retriever=index.as_retriever())\n",
    "\n",
    "def get_answer(query):\n",
    "  similar_docs = get_similiar_docs(query)\n",
    "  print(similar_docs)\n",
    "  answer = qa_chain.run(input_documents=similar_docs, query=query)\n",
    "  return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the SATA speeds?\"\n",
    "print(get_answer(query))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
